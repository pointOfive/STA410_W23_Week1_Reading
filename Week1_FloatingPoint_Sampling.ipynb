{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-TOC-FPN\"></a> \n",
        "\n",
        "### 0. [Computer Numbers](#cell-computer-numbers)\n",
        "0. [Floating-Point Numbers ${\\rm I\\!F}$](#cell-FPN)\n",
        "1. [Floating-Point Numbers ${\\rm I\\!F}$ in `Python`](#cell-FPN-py)\n",
        "  - [Special \"Numbers\" $\\pm\\infty$ and `NaN`](#cell-FPN-special)\n",
        "2. [Fixed-Point Numbers ${\\rm I\\!I}$](#cell-FxPN): Week 1 Programming Assignment Problem 1\n",
        "3. [(Pseudo)random Numbers](#cell-pseudo)  \n",
        "  0. [Bit Manipulation, Period, and Seed](#cell-pseudo) \n",
        "  1. [Modulus Recursion](#cell-mod-rec): Week 1 Programming Assignment Problem 2\n",
        "  2. [Generalized Feedback Shift Register Methods](#cell-mod-rec-gfsr)\n",
        "\n",
        "### 1. [Sampling](#cell-libs)\n",
        "0. [Beyond Uniform Sampling](#cell-rvs0)\n",
        "  0. [Inverse CDF Sampling](#cell-rvs)\n",
        "  1. [Rejection Sampling](cell-MCint-rejectionSampling)\n",
        "  2. [Importance Sampling](#cell-MCint-importanceSampling)\n",
        "1. [Integration by Sampling](#cell-INTest)\n",
        "  0. [Monte Carlo Integration](#cell-MCint)\n",
        "  1. [Improving Estimation](#cell-MCint-importanceSampling-VarRed)\n",
        "    - [More Importance Sampling](#cell-MCint-importanceSampling2)\n",
        "  2. [More Variance Reduction](#cell-MCint-importanceSampling-VarRed2) \n",
        "    - [Antithetic Sampling](#cell-MCint-importanceSampling-antithetic)\n",
        "    - [Control Variates](#cell-MCint-importanceSampling-variate)"
      ],
      "metadata": {
        "id": "DAG92bsZ5PJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Computer Numbers ([Return to TOC](#cell-TOC-FPN))\n",
        "---\n",
        "\n",
        "> *Computer numbers are not the same as real numbers, and the arithmatic operations on computer numbers are not exactly the same as those of ordinary arithmetic*. \n",
        ">\n",
        "> *The form of a mathematical expression and the way the expression should be evaluated in practice may be quite different.*\n",
        ">\n",
        "> -- James E. Gentle, Statistical Computing\n"
      ],
      "metadata": {
        "id": "VzwwwIbqXqoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-FPN\"></a> \n",
        "## 0.0 Floating-Point Numbers ${\\rm I\\!F}$  ([Return to TOC](#cell-TOC-FPN))\n",
        "---\n",
        "\n",
        "${\\rm I\\!F}$ is not ${\\rm I\\!R}$; but, if we employ ${\\rm I\\!F}$ carefully we can perform actual physical numerical computations in ${\\rm I\\!F}$ that can be substituted in place of their theoretical ${\\rm I\\!R}$ calculation counterparts.\n",
        "\n",
        "The ***floating-point numbers*** ${\\rm I\\!F}$ specification may be defined as follows.\n",
        "\n",
        "$$ \\overbrace{ \\left[ \\underbrace{ \\begin{array}{|c|} \\hline \\delta_0 \\\\ \\hline \\end{array}}_{\\text{sign}}\n",
        "\\underbrace{\\begin{array}{|c|c|c|c|c|c|} \\hline \\delta_1 & \\cdots & \\delta_{11}  \\\\ \\hline \\end{array}}_{\\text{$p$ binary representation }}\n",
        "\\underbrace{1.\n",
        " \\begin{array}{|c|c|c|} \\hline \\delta_{12}  & \\cdots & \\delta_{63} \\\\ \\hline \\end{array}}_{\\text{$s$ binary decimal fraction}} \\right] }^{\\Huge \\pm s \\times 2^p } $$    \n",
        "\n",
        "A 64-bit binary representation is based on the following.\n",
        "- a *sign* bit: $\\delta_0 \\longrightarrow \\pm$\n",
        "- *exponent* bits: $\\delta_1,  \\cdots, \\delta_{11} \\longrightarrow p$ in $2^p$ (where $2$ is the *base* or *radix*)\n",
        "\n",
        "  $$p = \\displaystyle \\sum_{i=0}^{10} \\delta_{11-i} 2^i - b$$\n",
        "\n",
        "  - $2^{11}=2048$ unique [exponent: $\\delta_1,  \\cdots, \\delta_{11}$] representations with `00000000000` $= 0$ to `11111111111` $= 2047$\n",
        "    \n",
        "  - the *bias* $b = $ `01111111111` $ = 1023 = \\sum_0^{9} 2^i = 2^{10}-1$ is the \"middle\" of the representation:\n",
        "    - `1??????????` $>$ `01111111111` and `0??????????` $\\leq$ `01111111111`\n",
        "    - $\\max p = 1024$ (although $1023$ would be symmetric: *more on this later*), $\\min p = -1023$, and $0$ is `01111111111` since `01111111111` $-$ `01111111111` $= 0$, i.e., \n",
        "\n",
        "      $$\\sum_0^{10} \\delta_{11-i} 2^i = b = \\sum_0^{9} 2^i \\quad \\Longrightarrow \\quad \\sum_0^{10} \\delta_{11-i} 2^i - b = 0$$\n",
        "  - the *range* is $\\require{cancel}$$\\pm2^p$ from $\\pm2^{-1023}$ to $\\pm2^{102\\overset{\\cancel{3}}{4}}$\n",
        "    - $\\pm2^{102\\overset{\\cancel{3}}{4}]} \\approx \\pm$`1.7976931348623157e+308` \n",
        "    - $\\pm2^{-1023} \\approx \\pm$`1.1125369292536007e-308` \n",
        "     \n",
        "    where the approximations above are *base $10$* representations which might feel more interpretable.\n",
        "\n",
        "- *significand/mantissa* bits: $s = 1.\\delta_{12}\\cdots\\delta_{63} \\in [1,2)$ are (*base $2$*) *binary decimal representations*\n",
        "  \n",
        "  $$ s = 2^0 + \\sum_{i=1}^{52} \\delta_{11+i} 2^{-i} \\quad \\text{ so that } \\quad x = \\pm \\, 2^p \\overbrace{\\left(2^0 + \\sum_{i=1}^{52} \\delta_{11+i} 2^{-i} \\right)}^{s}$$  \n",
        "  \n",
        "  The (*base $2$*) *binary decimal representation* $\\delta_{12}=\\delta_{13}=1, \\delta_{i>13}=0$ is $s=1.11000_\\cdots$ which is $1.75$ in *base $10$*\n",
        "  - $s$ is *normalized*, meaning in conjuction with $\\pm \\, 2^p$ it facilitates the scientific notation $\\pm s \\times 2^p$\n",
        "  - $1 \\leq s < $ *base*, so for *base $2$* then $1 \\leq s<2$; except,\n",
        "    - the *hiddent digit* `1.` is included unless all *exponent bits* are `0` \n",
        "    - so when the *exponent* $p=-1023$ the *hidden bit* \"disappears\" and the first *significand bit* $\\delta_{12}$ now multiplies $2^{-1}$ rather than $2^0$; the second *significand bit* $\\delta_{13}$ now multiplies $2^{-1}$ from $2^{-2}$; and in general the *significand bit* $\\delta_{11+i}$ now multiplies $2^{-i+1}$ rather than $2^{-i}$.\n",
        "\n",
        "   This *graceful underflow* extends the lower end of the *exponent range* from $p =$ `-1023` to $p =$ `-1023-51 = -1074` (since the last bit now corresponds to $2^{-51}$), thereby increasing the precision near `0`.\n",
        "  \n",
        "  - So actually, the limits for the *significand* $s$ are $2^{-51} \\leq s < 2$ when $p =$ `-1023` and the *hidden bit* is turned off for *graceful underflow*. \n",
        "\n",
        "Notice that when $p >$ `-1023` and the *significand* $s$ is not in the *graceful underflow* regime, the smallest contributing bit in the representation is $2^{-52} \\approx $ `2.220446049250313e-16`, which occurs when $\\delta_{63}=1$ and $\\delta_{12\\leq i<62}=0$.\n",
        "\n",
        "- However, this means that in this $16^{th}$ *base $10$* *decimal* location, the smallest represntable digit is `2` $>$ `1`.\n",
        "- Thus, not every digit can be represented in this $16^{th}$ *base $10$* *decimal* location; so, only the first 15 digits are *exactly correct*; and, thus, `resolution=1e-15` or `precision=15`."
      ],
      "metadata": {
        "id": "aZCgqJRA5Zuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-FPN-py\"></a> \n",
        "## 0.1 Floating-Point Numbers ${\\rm I\\!F}$ in `Python` ([Return to TOC](#cell-TOC-FPN))\n",
        "---"
      ],
      "metadata": {
        "id": "BmStYxQV8_Ws"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WWJSVW0v5B9_"
      },
      "outputs": [],
      "source": [
        "# You can look at the available resolution of floating-point numbers directly:\n",
        "print(2**-52, \": 16 total digits present\")\n",
        "print(2**-100, \": 16 total digits present, still\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can look at the available range directly (and break it):\n",
        "print(10.**308, ': \"Cool\", followed by \"Not Cool\"')\n",
        "print(10.**309, ': \"Not Cool\"')"
      ],
      "metadata": {
        "id": "TKim5q035pra"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# play around to find out the range limits and behavior:\n",
        "#2.**-(1023+52) # every single exponent and significand bit is 0\n",
        "#2.**-(1023+51) # all exponent bits are 0 but the last significand bit is 1\n",
        "#2.**-1023 # the first significand bits is 1 and all other significand bits are 0\n",
        "#2.**1024 # so actually p=1024 is the limit, not p=1023\n",
        "#2.**1023"
      ],
      "metadata": {
        "id": "bL7SyKrX5_P2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also just look up these specs:\n",
        "import numpy as np # give package a simpler standard alias\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.finfo.html\n",
        "np.finfo(float), np.finfo(float).precision"
      ],
      "metadata": {
        "id": "250NEXlz6Gqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can look at the bit string representations as well:\n",
        "# https://pypi.org/project/bitstring/\n",
        "! pip install bitstring # install a package not immediately available"
      ],
      "metadata": {
        "id": "xycezoASi_pR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitstring\n",
        "# Check out www.h-schmidt.net for an interactive 32-bit floating point demonstration.\n",
        "# https://stackoverflow.com/questions/16444726/binary-representation-of-float-in-python-bits-not-hex\n",
        "# i.e., bitstring.BitArray(float=1.0, length=32).bin\n",
        "print(bitstring.BitArray(float=2.5, length=64).bin)"
      ],
      "metadata": {
        "id": "qhPx1zk26TUb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's the graceful underflow from the hidden digit in action:\n",
        "print(bitstring.BitArray(float=2.**-(1023+0), length=64).bin)\n",
        "print(bitstring.BitArray(float=2.**-(1023+48), length=64).bin)"
      ],
      "metadata": {
        "id": "hnFR8MZg6zb7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some interesting floating point numbers:\n",
        "print(\"Largest possible 64-bit floating point number 1.7976931348623157e+308:\")\n",
        "print(bitstring.BitArray(float=1.7976931348623157e+308, length=64).bin)\n",
        "print(\" note the ^0^ there... interesting...\")\n",
        "\n",
        "print(\"\\n1:\", bitstring.BitArray(float=1., length=64).bin)\n",
        "print(\"0:\", bitstring.BitArray(float=0., length=64).bin)"
      ],
      "metadata": {
        "id": "al4PqRws645L"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-FPN-special\"></a> \n",
        "### $\\circ$ Special \"Numbers\" $\\pm\\infty$ and `NaN` ([Return to TOC](#cell-TOC-FPN))\n",
        "---\n",
        "\n",
        "Numeric values are not the only elements in the set of floating-point numbers. Namely, `NaN` and `Inf` are floating-point \"numbers\", too. It's just that `NaN` and `Inf` have *special reserved representations*. "
      ],
      "metadata": {
        "id": "WUSap6RQLF0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "type(0.), type(1.), type(2**-52), type(np.Inf), type(np.NaN) "
      ],
      "metadata": {
        "id": "VXny-Gl9rlX6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"0     \", bitstring.BitArray(float=0., length=64).bin)\n",
        "print(\"1     \", bitstring.BitArray(float=1., length=64).bin)\n",
        "print(\"np.Inf\", bitstring.BitArray(float=np.Inf, length=64).bin)\n",
        "print(\"np.NaN\", bitstring.BitArray(float=np.NaN, length=64).bin)\n",
        "# Do you see what the special reserved representations entail?"
      ],
      "metadata": {
        "id": "nUen_4NNLprb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *significand* bits are immaterial when representing `Inf`, so they get used to distinguish `NaN` from `Inf`. So, to represent `NaN`\n",
        "- all *exponent* bits are `1` (as with `Inf`)\n",
        "- and additionally at least one *signficand* bit (by default the first) is `1`\n",
        "\n",
        "  (as opposed to `Inf`, where all the `signficand` bits are `0`)\n",
        "\n",
        "Here are some examples of how Python handles these ***special numbers***."
      ],
      "metadata": {
        "id": "E3St_XGEsW2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"0    :\", bitstring.BitArray(float=0, length=64).bin)\n",
        "print(\"np.pi:\", np.pi)\n",
        "print(\"np.pi:\", bitstring.BitArray(float=np.pi, length=64).bin)\n",
        "\n",
        "print(\"\\n np.Inf         :\", bitstring.BitArray(float=np.Inf, length=64).bin)\n",
        "print(\" np.Inf + np.pi :\", bitstring.BitArray(float=np.Inf+np.pi, length=64).bin)\n",
        "print(\" np.Inf * np.pi :\", bitstring.BitArray(float=np.Inf*np.pi, length=64).bin)\n",
        "print(\" np.pi / np.Inf :\", bitstring.BitArray(float=np.pi/np.Inf, length=64).bin)\n",
        "print(\"-np.Inf         :\", bitstring.BitArray(float=-np.Inf, length=64).bin)\n",
        "print(\" 10**np.Inf     :\", bitstring.BitArray(float=10.**(np.Inf), length=64).bin)\n",
        "print(\" 10**-np.Inf    :\", bitstring.BitArray(float=10.**(-np.Inf), length=64).bin)\n",
        "\n",
        "print(\" np.Inf > np.pi :\", np.Inf > np.pi)\n",
        "print(\" np.Inf < np.pi :\", np.Inf < np.pi)\n",
        "print(\"-np.Inf < np.pi :\", -np.Inf < np.pi)\n",
        "\n",
        "print(\"\\n np.Inf - np.Inf :\", np.Inf-np.Inf)\n",
        "print(\" np.Inf / np.Inf :\", np.Inf/np.Inf)\n",
        "print(\" 0 * np.Inf      :\", 0*np.Inf)\n",
        "\n",
        "print(\"\\nnp.NaN          :\", bitstring.BitArray(float=np.NaN, length=64).bin)\n",
        "print(\"np.Inf          :\", bitstring.BitArray(float=np.Inf, length=64).bin)\n",
        "print(\"np.NaN + np.pi  :\", bitstring.BitArray(float=np.NaN+np.pi, length=64).bin)\n",
        "print(\"np.NaN + np.Inf :\", bitstring.BitArray(float=np.NaN+np.Inf, length=64).bin)\n",
        "print(\"np.NaN < np.pi  :\", np.NaN < np.pi)\n",
        "print(\"np.NaN < np.Inf :\", np.NaN < np.Inf)"
      ],
      "metadata": {
        "id": "I4Qc75QwsXiA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the `np.NaN` special number is only accessibly through `np.Inf` and `np.NaN` operations or direct specifications (e.g., as \"missing values\"). This is preferable since if numerical operations that are not defined we'd prefer for the operation to raise a useful error instead of just returning in `np.NaN`."
      ],
      "metadata": {
        "id": "xyXJWcLAs1dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "0. / 0."
      ],
      "metadata": {
        "id": "m0Qds5xls2Lt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-FxPN\"></a> \n",
        "## 0.2 Fixed-Point Numbers ${\\rm I\\!I}$  ([Return to TOC](#cell-TOC-FPN))\n",
        "---\n",
        "\n",
        "The material so far has focussed on ***floating-point numbers*** ${\\rm I\\!F}$ (which model, or approximate ${\\rm I\\!R}$). Only the *exponent* and the *sign bits* from ***floating-point numbers*** are needed to implement ***fixed-point numbers*** ${\\rm I\\!I}$ (which model, or approximate $\\mathbb{Z}$). However, there's still some room for interesting specifications for ${\\rm I\\!I}$, such as the *two's complement* representation of negative numbers (described in the last paragraph on page 86 in the James E. Gentle's **Computational Statistical** textbook).\n",
        "\n",
        "- You'll create a binary string representation for ***fixed-point numbers*** ${\\rm I\\!I}$ with a simple sign bit (rather than ***two's complement***) scheme to represent positive and negative integers in the [Week 1 Programming Portfolio Assignment]()."
      ],
      "metadata": {
        "id": "B7rK52T08FJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 (Pseudo)random Numbers ([Return to TOC](#cell-TOC-FPN))\n",
        "---\n",
        "\n",
        "Assuming the world isn't a simulation, we can observe ***real randomness*** in nature. Things that we might consider random are: \n",
        "- How many children does a person in a population have? \n",
        "- How many leaves does a tree in a forest have?\n",
        "- How much time is there between the arrival of two waves?\n",
        "- How many cars will we observe in a certation at a specific time of day?\n",
        "\n",
        "While such numbers can be measured and inputed to a computer for use (as is done by [random.org](https://www.random.org/)), most of the time when we say we're using \"random numbers\" in the context of a computer, we're actually using ***pseudorandom numbers***.\n",
        "\n",
        "***Pseudorandom numbers*** are deterministic sequences of numbers which exhibit apparent \"randomness\" to both the casual and critical observer.\n",
        "\n",
        "- From a programming perspective, the reproducibility pseudorandom number generation is a very useful property facilitating troubleshooting and testing."
      ],
      "metadata": {
        "id": "jM6mdSs9YCqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-pseudo\"></a>\n",
        "### 0.3.0 Bit Manipulation, Period, and Seed ([Return to TOC](#cell-TOC-FPN))\n",
        "--- \n",
        "\n",
        "***Floating point numbers*** ${\\rm I\\!F}$ (which model, or approximate ${\\rm I\\!R}$) and ***fixed-point numbers*** ${\\rm I\\!I}$ (which model, or approximate $\\mathbb{Z}$) are stored as ***bitstrings***.  To produce ***pseudorandom numbers***, then, ***bitstrings*** can be deterministically \"shuffled\" (or transformed) into other ***bitstrings***. This creates ***pseudorandom number sequences*** which will always be the same when started from the same ***bitstring***. Of course, the \"quality\" of the deterministically generated ***pseudorandom number sequences*** depends on the apparent \"randomness\" of the generated sequence. This sort of \"deterministic shuffling\" with ***bitstring manipulation*** is how ***pseudorandom numbers*** are created on computers.\n",
        "\n",
        "- The starting point of a ***pseudorandom number sequences*** is called the ***pseudorandom number seed***, or the ***random number seed***, or just the ***seed***.\n",
        "\n",
        "- Since computer number representations are finite, a deterministic function that produces the next number based on the current ***state*** will eventually complete its ***period*** and begin to cycle.\n",
        "\n",
        "  > A good ***pseudorandom number generator*** will produce ***pseudorandom number sequences*** with a long ***period*** so it doesn't provide cycling sequences of \"random\" numbers."
      ],
      "metadata": {
        "id": "TcsVkZWJBMo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-mod-rec\"></a>\n",
        "\n",
        "### 0.3.1 Modulus Recursion ([Return to TOC](#cell-TOC-FPN))\n",
        "---\n",
        "\n",
        "***Bitstring manipulation*** can be implicitly achieved with mathematical ***modulus recursion*** formulations such as \n",
        "\n",
        " $$u_k = au_{k-1} \\text{ mod } m$$\n",
        "\n",
        "which produces the sequence $\\{u_k: k=1,\\cdots,n\\}$ based on some ***seed*** $u_0$. The ***modulus recursion*** specification here is a ***first order sequential congruential method***.\n",
        "\n",
        "- Rather than leveraging a ***bitstring manipulation*** algorithm directly, in the [Week 1 Programming Assignment]() you'll create apparent \"randomness\" with ***modulus recursion*** and explore ***recursion***, ***pseudorandom number sequences***, and ***period*** etc. "
      ],
      "metadata": {
        "id": "PZ_MnZc9ZwrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-mod-rec-gfsr\"></a>\n",
        "\n",
        "### 0.3.2 Generalized Feedback Shift Register Methods ([Return to TOC](#cell-TOC-FPN))\n",
        "---\n",
        "\n",
        "A standard ***bitstring manipulation pseudorandom number generator*** is the ***Mersenne Twister***. The ***Mersenne Twister*** is a ***generalized feedback shift register (GFSR)*** method which produces ***bitstring manipulation*** with very good apparent \"randomness\" properties. The standard version of the ***Mersenne Twister*** is called `MT19937` because it has a ***period*** of $2^{19937-1}$ unique numbers before it begins to re-cycle through its ***pseudorandom number sequence***."
      ],
      "metadata": {
        "id": "Ya9NgnIMoxiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "# https://www.youtube.com/watch?v=_tN2ev3hO14?t881s\n",
        "# use the above link to start at t881s\n",
        "YouTubeVideo('_tN2ev3hO14', width=1024, height=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "i_O5NsrEBSVR",
        "outputId": "7db346e0-0ddf-4c4c-b0c2-1a8feb18476f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7feaa7f23f50>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1024\"\n",
              "            height=\"512\"\n",
              "            src=\"https://www.youtube.com/embed/_tN2ev3hO14\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> GFSR methods are described in more detail starting on page 16 of Keith Knight's STA410 [notes4.pdf](https://q.utoronto.ca/courses/296804/files?preview=24222606) document."
      ],
      "metadata": {
        "id": "6U9HiPR1qdrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-libs\"></a>\n",
        "# 1. Sampling ([Return to TOC](#cell-TOC-FPN))\n",
        "--- \n",
        "\n",
        "Random number generation for specific probability distributions in `Python` is available from the [pleasantly comprehensive](https://docs.scipy.org/doc/scipy/reference/stats.html) statistical library [`scipy.stats`](https://scipy.github.io/devdocs/tutorial/stats.html), which by default uses the [NumPy `rng`](https://scipy.github.io/devdocs/tutorial/stats.html#random-number-generation) for pseudorandom number generation. \n",
        " \n",
        "- Python's [`random`](https://docs.python.org/3/library/random.html) functionality \"uses the Mersenne Twister as the core generator\", but Numerical Python, (i.e., [NumPy](https://numpy.org/doc/stable/user/whatisnumpy.html)) [`np.random`](https://numpy.org/doc/stable/reference/random/#quick-start) functionality uses \"PCG64 which has better statistical properties than the legacy MT19937\" (which is nonetheless [still available](https://numpy.org/doc/stable/reference/random/bit_generators/mt19937.html)) as an option. \n",
        "\n",
        "> In Python, resources like `scipy.stats` and those previously used above are accessed with module imports, assuming the modules have been installed in the running Python environment. The first three `import` statements shown below were all run previously above, and the fourth import statement for `scipy.stats` is another form available for import statements. "
      ],
      "metadata": {
        "id": "_oKU_0QZBo0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bitstring # `! pip install bitstring` if this isn't yet installed\n",
        "# `pip install` will generally install anything you ask it to install.\n",
        "# More careful version capatiblity management is available with \n",
        "# `conda install` or `mamba install` (a faster compatibility solver).\n",
        "from IPython.display import YouTubeVideo # loading a function from a submodule\n",
        "import numpy as np # loading a module with a shorthand name\n",
        "from scipy import stats # loading just the submodule `stats`"
      ],
      "metadata": {
        "id": "vhODN9AbZQeQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-rvs0\"></a>\n",
        "## 1.0 Beyond Uniform Sampling ([Return to TOC](#cell-TOC-FPN))\n",
        "--- \n",
        "\n",
        "Analysis and testing (of the apparent \"randomness\") of ***pseudorandom numbers*** is a less active area of reasearch today than it once was, as \n",
        "***pseudorandom number generation*** from specific distributions for use by the practitioner is generally considered \"solved\" by libraries like [`scipy.stats`](https://scipy.github.io/devdocs/tutorial/stats.html). Thus, algorithmic details for sampling from specific important distributions (such as normal distributions) will be ignored in favor of more general sampling methods. Suffice it to say that much of the ***pseudorandom number generation*** provided by [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html) is created on the basis of ***bitstring*** \"shuffling\" followed by general methods that are able to repurpose these in a manner that makes them representative of samples from a chosen ***target distribtion***. \n",
        "\n",
        "> For some examples of sampling methods for normal distributions, please see pages 1-15 of Keith Knight's STA410 [notes6.pdf](https://q.utoronto.ca/courses/296804/files?preview=24222606) document, while the more general methods covered in the notes below are detailed in the [notes5.pdf](https://q.utoronto.ca/courses/296804/files?preview=24222587) and [notes4.pdf](https://q.utoronto.ca/courses/296804/files?preview=24222606) documents."
      ],
      "metadata": {
        "id": "t0Mdwcru39Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-rvs\"></a>\n",
        "### 1.0.0 Inverse CDF Sampling ([Return to TOC](#cell-TOC-FPN))\n",
        "--- \n",
        "\n",
        "The \"naive\" general solution to sampling from specified probability distributions is to apply the ***inverse CDF transformation*** to ***uniform random variables*** $U_i \\sim U(0,1)$ which are generated from ***bitstring manipulation*** based ***pseudorandom number generation*** algorithms such as `MT19937` and `PCG64`. Specifically, for ***inverse CDF transformation*** $x = P_X^{-1}(u)$ for the random variable $X$ \n",
        "\n",
        "$$Pr\\left(P_X^{-1}(U_i) \\leq x\\right) = Pr\\left(X_i \\leq x\\right) = P_X(x) \\quad \\Longrightarrow \\quad X_i \\sim p_X(x)$$\n",
        "\n",
        "\n",
        "|![](https://upload.wikimedia.org/wikipedia/commons/c/cc/Inverse_Transform_Sampling_Example.gif)|\n",
        "|-|\n",
        "|https://en.wikipedia.org/wiki/Inverse_transform_sampling|"
      ],
      "metadata": {
        "id": "mcvvcfxJZVhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "n=10000\n",
        "u,X = stats.uniform.rvs(size=n),stats.gamma(a=3)\n",
        "x = X.ppf(u)\n",
        "support = np.linspace(0,10,100)\n",
        "plt.hist(X.ppf(u), density=True, bins=100); plt.plot(support, X.pdf(support));"
      ],
      "metadata": {
        "id": "fWRVgK8OVLKs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-MCint-rejectionSampling\"></a>\n",
        "\n",
        "### 1.1.1 Rejection Sampling ([Return to TOC](#cell-TOC-FPN))\n",
        "\n",
        "---\n",
        "\n",
        "Another way to sample from $p_X(x)$ is to sample from a ***proposal distribution*** $\\tilde p_{\\tilde X} (\\tilde x)$ that is different than the ***target distribution*** $ p_{ X} ( x)$ and then subsequently correct the representation of the sampling to match the intended ***target distribution*** $p_{ X} ( x)$ distribution through a ***rejection sampling*** process.\n",
        "\n",
        "\n",
        "0. If there is some scaler $c$ such that $c \\times \\tilde p_{\\tilde X} ( x) > p(x)$ for all $x$ then proceed to step 1; otherwise, ***rejection sampling*** based on $\\tilde p_{\\tilde X}$ will not work\n",
        "1. Sample $\\tilde X_j \\sim \\tilde p_{\\tilde X} (\\tilde x)$ \n",
        "2. Keep $X_i=\\tilde X_j$ with probability $\\frac{p_{ X}(\\tilde X_j)}{c \\tilde p_X (\\tilde X_j)}$\n",
        "3. Discard $\\tilde X_j$ if it was not kept, and return to step 1 as desired \n",
        "\n",
        "> ***Rejection sampling*** turns a univariate sampling problem $X_i \\sim p_X(x)$ into a two-dimensional problem $(\\tilde X_j,U_j) \\sim \\tilde p(\\tilde x,u) = \\tilde p(\\tilde x) \\tilde p(u)$ where the conditional distribution\n",
        ">\n",
        "> $$\\tilde p\\left(X\\big| U < \\frac{p_X(X)}{c \\tilde p_{\\tilde X}(X)} \\right) = p_X(X)$$\n",
        ">\n",
        "> is identical to the distribution from which sampling is required. "
      ],
      "metadata": {
        "id": "zk0CYfq1lNMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of interest\n",
        "X = stats.beta(2,2)\n",
        "support = np.linspace(0,1,100)\n",
        "plt.plot(support, X.pdf(support))\n",
        "\n",
        "# and distribution tilde_p(tilde_x) such that c*tilde_p(x)>p(x) for all x\n",
        "tilde_X = stats.uniform()\n",
        "n = 1000\n",
        "tilde_x = X.rvs(size=n)\n",
        "# along with other \n",
        "u = stats.uniform.rvs(size=n)\n",
        "\n",
        "c=1.6\n",
        "plt.plot(support, c*tilde_X.pdf(support)); plt.xlabel('X'); plt.ylabel('U')\n",
        "plt.scatter(tilde_x, c*u, color=['b' if u<(X.pdf(x)/c*tilde_X.pdf(x)) else 'r' for x,u in zip(tilde_x,u)]);"
      ],
      "metadata": {
        "id": "_D-JcBlqltdn"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-MCint-importanceSampling\"></a>\n",
        "\n",
        "### 1.1.2 Importance Sampling ([Return to TOC](#cell-TOC-FPN))\n",
        "\n",
        "---\n",
        "\n",
        "Rather than filtering the samples of a ***proposal distribution*** to create a sample set reflecting the ***target distribution*** of interest $X_i \\sim p(x)$ as in ***rejection sampling***, all of the samples of the ***proposal distribution*** $\\tilde X_i \\sim \\tilde p(x)$ can be used in a weighted manner according to ***importance weights*** \n",
        "\n",
        "$$W_i^* = \\frac{p(\\tilde X_i)}{\\tilde p(\\tilde X_i)} \\quad \\text{ and } \\quad W_i = \\frac{ W_i^*}{\\sum_{i=1}^n  W_i^*}$$\n",
        "\n",
        "where the $W_i$ weighted random variables $\\tilde X_i$ are used in place of random variables $X_i \\sim p(x)$.\n",
        "\n",
        "If only $q(x)$ proportional to the desired ***target distribution*** $p(x) = cq(x)$ is known, ***importance sampling*** can still produce samples from $p(x)$ because the unknown proportionality constant $c$ will cancel in the normalization $\\require{cancel}$ since\n",
        "\n",
        "$$W_i = \\frac{ W_i^*}{\\sum_{i=1}^n  W_i^*} = \\frac{ p(\\tilde X_i)/\\tilde p(\\tilde X_i)}{\\sum_{i=1}^n  p(\\tilde X_i)/\\tilde p(\\tilde X_i)} = \\frac{ \\cancel{c}q(\\tilde X_i)/\\tilde p(\\tilde X_i)}{\\sum_{i=1}^n  \\cancel{c}q(\\tilde X_i)/\\tilde p(\\tilde X_i)}$$\n",
        "\n",
        "> In Bayesian analysis a posterior can be evaluated proportionally as\n",
        "> \n",
        "> $$p(\\theta | \\mathbf{x}) \\propto f(\\mathbf{x}|\\theta) p(\\theta) $$\n",
        "> \n",
        "> but usually not exactly because the (marginal likelihood) normalizing constant $f(\\textbf{x})$ tends to be very expensive to compute. \n",
        "> \n",
        "> Peforming ***importance sampling*** for a posterior distribution as the ***target distribution*** with the prior distribution as the ***proposal distribution*** $p(\\theta)$ leads to ***importance weights*** which are simply the normalized values of the likelihood \n",
        "> \n",
        "> \\begin{align*}\n",
        "W_i = {} & \\frac{f(\\mathbf{x}|\\theta_i)}{\\sum_i f(\\mathbf{x}|\\theta_i)} \\; \\text{ since} \\\\\n",
        "W_i^* = {} & \\frac{p(\\theta_i | \\mathbf{x})}{p(\\theta_i)} = \\frac{f(\\mathbf{x}|\\theta_i) p(\\theta_i)\\overset{\\text{normalization}}{/ \\cancel{f(\\mathbf{x})}}}{p(\\theta_i)} = f(\\mathbf{x}|\\theta_i)\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "yz3hsl8SlwlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the importance sampling distribution tilde_p(tilde_x)\n",
        "tilde_X = stats.uniform()\n",
        "n = 100000\n",
        "tilde_x = tilde_X.rvs(size=n)\n",
        "\n",
        "# the target distribution p(x)\n",
        "X = stats.beta(2,2)\n",
        "w = X.pdf(tilde_x)/tilde_X.pdf(tilde_x)\n",
        "w = w/w.sum()\n",
        "\n",
        "support = np.linspace(0,1,100)\n",
        "plt.hist(tilde_x, density=True, bins=50, alpha=0.25)\n",
        "plt.plot(support, tilde_X.pdf(support))\n",
        "\n",
        "plt.hist(tilde_x, weights=w, density=True, bins=50, alpha=0.5)\n",
        "plt.plot(support, X.pdf(support));"
      ],
      "metadata": {
        "id": "RXb16Wyclv_y"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-INTest\"></a>\n",
        "\n",
        "# 1.1 Integration by Sampling ([Return to TOC](#cell-TOC-FPN))\n",
        "\n",
        "---\n",
        "\n",
        "If we can write\n",
        "\n",
        "$$ \\int f(x) dx \\; = \\int g(x)p(x) dx \\quad \\text{ then } \\quad \\int f(x) dx = E_{X\\sim p_X(x)}[g(x)]$$\n",
        "\n",
        "so  \n",
        "\n",
        "$$\\hat \\theta \\; \\text{ estimating }\\; E_X[g(x)] \\quad \\text{ also estimates } \\quad \\int f(x) dx$$\n",
        "\n",
        "If the ***central limit theorem*** for $\\hat \\theta$ estimating $E_X[g(x)]$ applies, then the ***standard error*** of the estimator defines how accurate our ***integral estimation*** is."
      ],
      "metadata": {
        "id": "5AaF6ARP03Jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-MCint\"></a>\n",
        "\n",
        "## 1.1.0 Monte Carlo Integration ([Return to TOC](#cell-TOC-FPN))\n",
        "\n",
        "---\n",
        "\n",
        "Since \n",
        "\n",
        "$$\\int_a^b \\!\\!\\!\\! f(x) \\frac{1}{b-a} dx = E_X[f(X)] \\text{ for } X\\sim U(a,b)$$\n",
        "\n",
        "if \n",
        "\n",
        "$$Y_i = (b-a)f(X_i) \\quad \\text{ then } \\quad E_X\\left[Y_i\\right] \\; = \\int_a^b \\!\\!\\!\\!(b-a) f(x) \\frac{1}{(b-a)} dx \\; = \\int_a^b \\!\\!\\!\\!f(x) dx \\quad \\text{ and } \\quad  \\bar Y_n \\sim N\\left(\\int_a^b \\!\\!\\!\\! f(x)  dx ,\\frac{Var[Y]}{n} \\right)$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\frac{\\bar Y_n - \\int_a^b \\!\\!f(x) dx}{\\sqrt{\\widehat{Var[Y]/n}}} \\sim t_{n-1}$$\n",
        "\n",
        "so \n",
        "\n",
        "$$\\text{the integration } \\quad \\displaystyle \\int_a^b \\!\\!\\!\\!f(x) dx \\quad \\text{ is estimated by } \\quad \\bar Y_n = \\frac{\\sum_{i=1}^n Y_i}{n} \\quad \\text{ where } \\quad Y_i = (b-a)f(X_i) \\; \\text{ and } \\; X_i\\sim U(a,b) $$\n",
        "\n",
        "|![](https://enricodegiuli.files.wordpress.com/2017/05/pi_30k.gif?resize=500%2C500)|\n",
        "|-|\n",
        "|https://en.wikipedia.org/wiki/Monte_Carlo_method|\n"
      ],
      "metadata": {
        "id": "2XH_hsQl1Kpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.integrate import quad\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "n = 50\n",
        "for i in range(500):\n",
        "  mcint = (stats.expon(scale=1/5).pdf(stats.uniform.rvs(size=n))**2).cumsum()/np.arange(1,n+1)\n",
        "  plt.plot(mcint[10:], 'b:', alpha=0.5)\n",
        "  mcint = (stats.expon(scale=1/5).pdf(stats.truncexpon(5, scale=1/5).rvs(size=n)))\n",
        "  mcint = mcint.cumsum()/np.arange(1,n+1)\n",
        "  plt.plot(mcint[10:], \"y:\", alpha=0.5)\n",
        "  \n",
        "mcint = (stats.expon(scale=1/5).pdf(stats.uniform.rvs(size=n))**2).cumsum()/np.arange(1,n+1)\n",
        "plt.plot(mcint[10:], c='b', label=\"expectation of squared exponential using standard unit uniform\")\n",
        "mcint = (stats.expon(scale=1/5).pdf(stats.truncexpon(5, scale=1/5).rvs(size=n)))\n",
        "mcint = mcint.cumsum()/np.arange(1,n+1)\n",
        "plt.plot(mcint[10:], c='y', label=\"expectation of exponential using truncated exponential\")\n",
        "plt.plot((n-10)*[quad(lambda x: stats.expon(scale=1/5).pdf(x)**2, a=0,b=1)[0]], c='red', label=\"truth\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "cS3UEE015xsm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-MCint-importanceSampling-VarRed\"></a>\n",
        "\n",
        "## 1.2.1 Improving Estimation ([Return to TOC](#cell-TOC-FPN))\n",
        "----\n",
        "\n",
        "Consider the two following integral estimations, which are equivalent by [the law](https://math.stackexchange.com/questions/415196/proving-the-law-of-the-unconscious-statistician) of the [unconscious statistician](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician)\n",
        "\n",
        "$$\\int_0^1 5e^{-5x}5e^{-5x} dx = E_{X\\sim \\text{uniform}(0, 1)}[25e^{-10x}]= E_{X\\sim \\text{truncexpon}(5, 1)}[5e^{-5x}] $$ \n",
        "\n",
        "If by sampling from some distribution other than a uniform distribution as in ***Monte Carlo integration***\n",
        "- computations of $g(x)\\approx 0$ which do not meaningfully contribute to the integral are avoided, while\n",
        "- computations of $|g(x)| > 0$ which  meaningfully contribute to the integral are enriched\n",
        "\n",
        "then the efficiency of the estimation of the integral $\\int \\!\\!g(x)p(x) dx$ will be improved.  \n",
        "\n",
        "> This can be understood as reducing the variation of the \"sampled pdf\" values\n",
        ">\n",
        "> $\\;\\;$ `np.var(expon(scale=1/5).pdf(truncexpon(5, scale=1/5).rvs(size=n))))` \n",
        ">\n",
        "> $<$ `np.var(expon(scale=1/5).pdf(uniform.rvs(size=n))**2)`\n",
        ">\n",
        "> while still maintaining the desired expected value.\n",
        "\n",
        "\n",
        "Thus, effective use of ***inverse CDF sampling***, ***rejection sampling***, and ***importance sampling*** can provide increased computational efficiency for integral estimation. "
      ],
      "metadata": {
        "id": "ZtnAXTjc3pPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.linspace(0,1,100)\n",
        "plt.plot(x, 25*np.exp(-10*x));"
      ],
      "metadata": {
        "id": "phNv5Nk45xT2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"By sampling in a proportional manner to the density evalutions\")\n",
        "print(\"the variance of the resulting 'pdf samples'\")\n",
        "print(np.var(stats.expon(scale=1/5).pdf(stats.truncexpon(5, scale=1/5).rvs(size=n))))\n",
        "print(\"is much less than\")\n",
        "print(np.var(stats.expon(scale=1/5).pdf(stats.uniform.rvs(size=n))**2))\n",
        "print(\"the variance of the 'pdf samples' that do not try to minimize\")\n",
        "print(\"the presence of 'extreme value' 0's\")"
      ],
      "metadata": {
        "id": "uucEqPX33x0Y"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-MCint-importanceSampling2\"></a>\n",
        "\n",
        "### More Importance Sampling ([Return to TOC](#cell-TOC-FPN))\n",
        "---\n",
        "\n",
        "The example above demonstrates that estimation of\n",
        "\n",
        "$$\\int g(x) p(x) dx$$\n",
        "\n",
        "is most efficient if $p(x) \\propto g(x)$. When the available factorization $g(x)p(x)$ does not lend itself to this objective, ***importance sampling*** based on\n",
        "\n",
        "$$\\int g(x) p(x) dx = \\int g(x) \\frac{p(x)}{\\tilde p(x)} \\tilde p(x) dx$$\n",
        "\n",
        "can help improve the proportionality by and thereby reduce the variance of the estimation.\n",
        "\n",
        "> ***Importance sampling*** is effective at reducing sampling variance by choosing $\\tilde p(x)$ which\n",
        "> \n",
        "> 1. rarely samples $x$ where $g(x)\\approx 0$ so $\\left(g(x) \\frac{p(x)}{\\tilde p(x)} \\right)$ remains large\n",
        "> 2. has heavier tails than $p(x)$ so the weights $W_i^* = \\frac{p(x)}{\\tilde p(x)}$ do not explode\n",
        ">\n",
        "> so that $\\left(g(x) \\frac{p(x)}{\\tilde p(x)} \\right)$ is effectively bounded and therefore has reduced variance.\n",
        "> ***Importance sampling*** thus oversamples $\\left(g(x) \\frac{p(x)}{\\tilde p(x)} \\right)$ values which dominate the integration so that the function is better represented and more precisely estimated in the key areas, but then appropriately downweighting the oversampling to maintain the correct expected value.\n",
        "> \n",
        "> - When ***importance sampling***  is working well the ***importance weights*** will be relatively balanced so that the full diversity of the ***importance samples*** meaningfully contribute to the integral estimate, and the ***effective sample size*** remains close to the number of ***importance samples*** being used.\n",
        "\n",
        "For a posterior distribution $p(\\theta|\\mathbf{x}) = \\text{Beta}(2,2)$, the posterior probability\n",
        "\n",
        "$$\\Pr(0.95 < \\theta < 0.99 | \\mathbf{x}) = E_{\\theta|\\mathbf{x}}[1_{[.95,.99]}(\\theta)] = \\int 1_{[.95, .99]}(\\theta) p(\\theta|\\mathbf{x}) d\\theta$$\n",
        "\n",
        "will not be most efficiently calculated by naively basing estimation on samples from $p(\\theta|\\mathbf{x}) = \\text{Beta}(2,2)$. Instead, ***importance sampling*** from a uniform distribution (which is equivalent to ***Monte Carlo integration***) biases samples to be more useful for the integration estimation, by attempting make the proportionality \n",
        "$g(x) \\frac{p(x)}{\\tilde p(x)} \\propto \\tilde p(x)$ better than $g(x) \\propto p(x)$ so that $\\text{Var}_{\\tilde p(x)}(g(x) \\frac{p(x)}{\\tilde p(x)}) < \\text{Var}_{p(x)}(g(x))$."
      ],
      "metadata": {
        "id": "lCPbPaTZ5cfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = stats.beta(2,2)\n",
        "tilde_X = stats.uniform()\n",
        "n = 10000\n",
        "tilde_x = tilde_X.rvs(size=n) \n",
        "x = X.rvs(size=n)\n",
        "w_ = X.pdf(tilde_x)/tilde_X.pdf(tilde_x)\n",
        "w = w_/w_.sum() # = X.pdf(tilde_x) in this case\n",
        "\n",
        "def region(x, a=.95,b=.99):\n",
        "  return (x>a)&(x<b)\n",
        "\n",
        "print(\"The Analytical Truth is: \", X.cdf(.99)-X.cdf(.95))\n",
        "print(\"Naive Beta(2,2) Samples: \", np.mean(region(x)))\n",
        "print(\"Importance Sampling V1: \", np.mean(region(tilde_x)*w_))\n",
        "print(\"Importance Sampling V2: \", (w*region(tilde_x)).sum())"
      ],
      "metadata": {
        "id": "oPXhWVZeBYnj"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The variances on the samples used to estimate the integration are\")\n",
        "print(\"Naive Beta(2,2) Samples: \", np.var(region(x)))\n",
        "print(\"Importance Sampling V1: \", np.var(region(tilde_x)*X.pdf(tilde_x)))\n",
        "print(\"Importance Sampling V2: \", (w*(region(tilde_x)-(w*region(tilde_x)).sum())**2).sum()) # np.cov(region(tilde_x), aweights=w)"
      ],
      "metadata": {
        "id": "E0WPo9qaGYEM"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why are there V1 and V2 versions of Importance Sampling?\n",
        "# Notice how their estimates are highly correlated but their variances are quite different!"
      ],
      "metadata": {
        "id": "4KNTRfN0DE6B"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When estimating the integral using ***importance sampling***\n",
        "\n",
        "\\begin{align*}\n",
        "\\int g(x) p(x) dx & = {} \\int g(x) \\frac{p(x)}{\\tilde p(x)} \\tilde p(x) dx\\\\\n",
        "E_{p(x)}[g(x)] & = {} E_{\\tilde p(x)}\\left[g(x) \\frac{p(x)}{\\tilde p(x)} \\right]\n",
        "\\end{align*}\n",
        "\n",
        "the prescribed estimator is actually\n",
        "\n",
        "$$\\overline{W^* Y} =  \\underbrace{\\frac{\\sum_{i=1}^n W_i^* Y_i}{n}}_{E_{\\tilde p(x)}[W_i^*]=1} \\quad \\text{ as opposed to } \\quad \\overline{Y} = \\underbrace{\\sum_{i=1}^n W_i Y_i}_{E_{\\tilde p(x)}[W_i]=\\frac{1}{n}}$$\n",
        "\n",
        "$$\\text{where } \\quad Y_i = g(\\tilde X_i) \\quad \\tilde X_i \\sim \\tilde p(x) \\quad  W_i^* = \\frac{p(\\tilde X_i)}{\\tilde p(\\tilde X_i)} \\quad W_i = \\frac{ W_i^*}{\\sum_{i=1}^n  W_i^*}$$\n",
        "\n",
        "> The former estimator is \"recommended\" because the latter is slighly biased since it can be shown that\n",
        ">\n",
        "> $$E[\\overline{W^* Y}] =  E_{p(x)}[g(x)] \\quad \\text{ but } \\quad E[\\overline{Y}] = E[\\overline{W^* Y}] \\left(1 + \\frac{1}{n}Var(W_i^*)\\right) - \\frac{1}{n}Cov(W_i^*Y_i, W_i^*) + \\mathcal{O}(n^{-2})$$\n",
        "> \n",
        "> However, the bias can be estimated (and adjusted) with $\\widehat{\\text{Var}}(W_i^*)$ and $\\widehat{\\text{Cov}}(W_i^*Y_i, W_i^*)$; and, it can additionally be shown that the ***mean squared error*** \n",
        ">\n",
        "> $$\\text{E}[(\\overline{Y} - \\text{E}[\\overline{Y}])^2] = \\text{Var}(\\overline{Y}) + \\text{bias}[\\overline{Y}]^2 <  MSE(\\overline{W^* Y})$$\n",
        ">\n",
        "> whenever\n",
        ">\n",
        "> $$\\quad \\text{ whenever } \\quad \\text{Cor}(W_i^*Y_i, W_i^*) > \\frac{\\text{cv}(W^*_i)}{2\\text{cv}(Y_i)}$$\n",
        ">\n",
        ">   where, e.g., $\\text{cv}(Y_i) = \\frac{\\sigma_{Y_i}}{\\mu_{Y_i}}$ is the ***coefficient of variation***."
      ],
      "metadata": {
        "id": "O1DfbHSwrrni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_wy, cov_wy_w, cov_wy_w, var_w = np.cov(w*region(tilde_x), w).flatten()\n",
        "mean_wy, mean_w = np.mean(w*region(tilde_x)), np.mean(w)\n",
        "print(\"     bias\", \"*\",(1+var_w/n), '-', cov_wy_w/n, '+ O(', n**-2, ')')\n",
        "\n",
        "print(\"\\ncor(YW*, W*)\", cov_wy_w/(np.sqrt(var_wy*var_w)))\n",
        "print(\">\")\n",
        "print(\"cv(W*)/(2cv(YW*)\", (np.sqrt(var_w)/mean_w)/(2*np.sqrt(var_wy)/mean_wy))\n",
        "print(\"?\\n\\nIf not, the MSE for Importance Sampling V1 should be smaller\")\n",
        "print(\"than the MSE for Importance Sampling V2, as it is\")"
      ],
      "metadata": {
        "id": "I8lgPqlfBNRV"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Returning to our two estimators $\\overline{W^* Y}$ and $\\overline{Y}$, for the two samples \n",
        "> \n",
        "> 1. $\\{X_i = x_i: \\; X_i \\sim p(x) \\text{ for } i=1,\\cdots, n\\}$ \n",
        "> \n",
        "> 2. $\\{\\tilde X_i = \\tilde x_i: \\; \\tilde X_i \\sim \\tilde p( \\tilde x) \\text{ for } i=1,\\cdots, n\\}$\n",
        "> \n",
        "> and two sets of weights\n",
        "> 1. $\\left\\{w_i^* = \\frac{p(x_i)}{\\tilde p(x_i)} : i=1,\\cdots, n \n",
        "> \\right\\}$ \n",
        "> 2. $\\left\\{w_i = \\frac{w_i^*}{\\sum_{i=1}^n w_i^*} : i=1,\\cdots, n \\right\\}$\n",
        "> \n",
        "> the three estimands\n",
        "> \n",
        "> 1. $\\overline{g(x)} = \\frac{\\sum_{i=1}^n g(x_i)}{n} $\n",
        "> 2. $\\overline{w^*y}= \\frac{\\sum_{i=1}^n w_i^*y_i}{n}\\quad \\text{ where } \\quad  y_i= g(\\tilde x_i)$\n",
        "> 3. $\\bar y = \\sum_{i=1} w_i y_i \\quad \\text{ and still } \\quad y_i= g(\\tilde x_i) $\n",
        "> \n",
        "> all estimate $\\displaystyle \\int \\!\\! g(x) p(x) dx = \\int \\!\\! g(x) \\frac{p(x)}{\\tilde p(x)} \\tilde p(x) dx$ and have observed sample variances \n",
        "> \n",
        "> 1. $\\hat \\sigma^2_p = \\frac{\\sum_{i=1}^n(g(x_i)-\\overline{g(x)})^2}{n-1}$\n",
        "> 2. $\\hat \\sigma^2_{\\tilde p} = \\frac{\\sum_{i=1}^n(w_i^*y_i-\\overline{w^*y})^2}{n-1}$\n",
        "> 3. $\\hat \\sigma^2_{w} = \\sum_{i=1}^n w_i (y_i - \\bar y)^2$\n",
        "> \n",
        "> with \n",
        "> \n",
        "> - $\\hat \\sigma^2_{\\tilde p} < \\hat \\sigma^2_{p}$ if relative sampling correction factor $w_i^* = \\frac{p(\\tilde x_i)}{\\tilde p(\\tilde x_i)}$ tends to be larger (smaller) for smaller (larger) $y_i = g(\\tilde x_i)$ values so\n",
        ">\n",
        ">   $$\\frac{\\sum_{i=1}^n(w_i^*y_i-\\overline{w^*y})^2}{n-1} < \\frac{\\sum_{i=1}^n(g(x_i)-\\overline{g(x)})^2}{n-1}$$\n",
        "> \n",
        "> - $\\hat \\sigma^2_{w} < \\hat \\sigma^2_p$ if extreme $y_i = g(\\tilde x_i)$ values tend to have smaller oversampling correction factors $w_i = \\frac{p(\\tilde x_i)}{\\tilde p(\\tilde x_i)} \\big / \\! \\sum_{i=1}^n \\!\\frac{p(\\tilde x_i)}{\\tilde p(\\tilde x_i)}$ so \n",
        "> \n",
        "> $$\n",
        "\\begin{align*}\n",
        "\\sum_{i=1}^n w_i (y_i - \\bar y)^2 = & {} \\sum_{i=1}^n \\left(\\frac{p(\\tilde x_i)}{\\tilde p(\\tilde x_i)} \\big / \\! \\sum_{i=1}^n \\!\\frac{p(\\tilde x_i)}{\\tilde p(\\tilde x_i)} \\right) (g(\\tilde x_i) - \\overline{g(\\tilde x)})^2  \\\\\n",
        "< & {}\\sum_{i=1}^n \\frac{1}{n-1} (g( x_i) - \\overline{g( x)})^2\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "axMzdC2Vytzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-MCint-importanceSampling-VarRed2\"></a>\n",
        "\n",
        "## 1.2.2 More Variance Reduction ([Return to TOC](#cell-TOC-FPN))\n",
        "----\n",
        "\n",
        "The previous two examples show that integral estimation is made more efficient by biasing samples towards the most relevant (non-zero) regions of the integration (while appropriately adjusting the integrand so that the expected value remains correct). \n",
        "\n",
        "Sometimes the variance can be further reduced by identifying and leveraging available correlations present in collections of estimators."
      ],
      "metadata": {
        "id": "bSZ9-Kwi6g0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-MCint-importanceSampling-antithetic\"></a>\n",
        "\n",
        "\n",
        "### Antithetic Sampling ([Return to TOC](#cell-TOC-FPN))\n",
        "\n",
        "---\n",
        "\n",
        "Suppose $\\bar x_1$ and $\\bar x_2$ are both unbiased estimators of the same target, and have correlation $\\rho = \\frac{Cov(\\bar x_1+\\bar x_2)}{\\sigma_{\\bar x_1}\\sigma_{\\bar x_2}}$. If (for simplicity) $\\sigma_{\\bar x_1}^2=\\sigma_{\\bar x_2}^2=\\frac{\\sigma^2}{n}$, then\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "Var\\left(\\frac{\\bar x_1+\\bar x_2}{2}\\right) = {} & \\frac{Var(\\bar x_1)+Var(\\bar x_2)}{4} + \\frac{2Cov(\\bar x_1+\\bar x_2)}{4} \\\\\n",
        "= {} & \\frac{\\sigma^2}{2n} + \\frac{\\frac{\\sigma}{\\sqrt{n}}\\frac{\\sigma}{\\sqrt{n}}\\rho}{2} = \\frac{(1+\\rho)\\sigma^2}{2n}\n",
        "\\end{align*}$$\n",
        "\n",
        "So if $\\rho<0$ the averaged estimator could have substantially lower variance than either of the individuals estimators, and $\\rho<0$ is not as hard to achieve as might initially be imagined, e.g., \n",
        "- $\\rho_{\\bar x_1,\\bar x_2} < 0$ if $x_{2i} = -x_{1i}$ for $x_{1i}\\sim N(\\mu, \\sigma^2)$\n",
        "- $\\rho_{\\bar x_1,\\bar x_2} < 0$ if $x_{2i} = 1-x_{1i}$ for $x_{1i}\\sim U(0,1)$\n",
        "\n",
        "\n",
        "> ***Antithetic sampling*** is to some degree what motivates the ***Bagging*** of ***Random Forests*** where for tree-based precictions $t_k, j=1,\\cdots, K$ with a common variance $\\sigma^2_{t}$ and shared correlation $\\rho = \\frac{Cov(t_k,t_{k'})}{\\sigma^2_{t}}$\n",
        "> $$\n",
        "\\begin{align*}Var(\\bar t) = {} & \\frac{1}{K^2} \\sum_{k=1}^K \\sigma^2_t + \\frac{2}{K^2} \\sum_{k,k'} \\sigma_t^2 \\rho\\\\\n",
        "= {} & \\frac{\\sigma^2}{K} + \\frac{\\frac{2}{2}(K^2-K)\\sigma_t^2\\rho}{K^2}\n",
        " = \\rho \\sigma^2 + \\frac{(1-\\rho)\\sigma^2}{K}\n",
        "\\end{align*}$$\n",
        "> where gains from averaging for even positively correlated estimators will be beneficial.\n",
        "\n",
        "<!-- = \\frac{\\sigma^2}{K} + \\sigma_t^2\\rho - \\frac{\\sigma_t^2\\rho}{K} -->"
      ],
      "metadata": {
        "id": "YbYJRCb-KjZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's an example using importance sampling, but notice\n",
        "# that the antithetical sampling is a general strategy that \n",
        "# could be used in other contexts besides importance sampling\n",
        "tilde_X = stats.uniform()\n",
        "X = stats.beta(2,2)\n",
        "n = 10000\n",
        "tilde_x = tilde_X.rvs(size=n) \n",
        "x = X.rvs(size=n)\n",
        "w = X.pdf(tilde_x)/tilde_X.pdf(tilde_x)\n",
        "\n",
        "print(\"True Value \", X.cdf(.99)-X.cdf(.95))\n",
        "print(\"Importance Sampling Estimate\")\n",
        "print(\"           \", (w*region(tilde_x)).mean())\n",
        "# Antithetical Sampling\n",
        "tilde_x_1 = w*region(tilde_x)\n",
        "tilde_x_2 = w*region(1-tilde_x)\n",
        "tilde_xs_ave = 0.5*(tilde_x_1.mean()+tilde_x_2.mean())\n",
        "print(\"Importance+Antithetical Sampling  Estimate\")\n",
        "print(\"           \", tilde_xs_ave)"
      ],
      "metadata": {
        "id": "B16043s7N7wL"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Importance Sampling Variance\")\n",
        "print(np.var(w*region(tilde_x)))\n",
        "print(\"Importance+Antithetical Sampling Variance\")\n",
        "# here's one way to estimate the variance of the Antithetical Sampling estimate\n",
        "tilde_xs_var = 0.25*(np.var(tilde_x_1)/n + \n",
        "                     np.var(tilde_x_2)/n + \n",
        "                     (2/n**2)*np.cov(tilde_x_1, tilde_x_2)[0,1])\n",
        "print(tilde_xs_var*n) # n factor to rescale to single sample"
      ],
      "metadata": {
        "id": "Yt48xRlsONv7"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"cell-MCint-importanceSampling-variate\"></a>\n",
        "\n",
        "### Control Variates ([Return to TOC](#cell-TOC-FPN))\n",
        "\n",
        "---\n",
        "\n",
        "If correlation between estimators is known, regardless of the target of the estimators, the correlation may be exploited whenever the true value of auxiliary estimators is known.\n",
        "\n",
        "For any estimator $\\hat \\theta$ of interest and any other correlated estimator $\\hat \\lambda$\n",
        "\n",
        "- where the estimators need not even estimate the same target, e.g.,\n",
        "  $$E[\\hat \\theta] = \\theta  \\not = \\lambda = E[\\hat \\lambda]$$\n",
        "- and where $\\lambda$ and $Cov(\\hat \\theta, \\hat \\lambda)$ must be known so that\n",
        "\n",
        "  1. since $Cov(\\hat \\theta, \\hat \\lambda)$ is known the relative value of $\\hat \\lambda$ to $\\lambda$ is informative of the relative value of $\\hat \\theta$ to $\\theta$\n",
        "     - e.g., for $Cov(\\hat \\theta, \\hat \\lambda)>0$, then if $\\hat \\lambda > \\lambda$ it is likely that $\\hat \\theta > \\theta$ (and if $\\hat \\lambda < \\lambda$ it is likely that $\\hat \\theta < \\theta$)\n",
        "     \n",
        "  2. the estimator $\\hat \\theta + \\underset{- \\text{ when } \\hat \\lambda > \\lambda}{\\overset{+ \\text{ when } \\hat \\lambda > \\lambda}{\\beta (\\hat \\lambda-\\lambda)}}$ can be used to correct for expected under and over estimates in $\\hat \\theta$ based on observed under and over estimates of $\\hat \\lambda$ compared with $\\lambda$.\n",
        "\n",
        "Specifically, when the above requirements are met\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "Var\\left(\\hat \\theta + \\beta (\\hat \\lambda-\\lambda)\\right) ={} & \\overbrace{Var\\left(\\hat \\theta \\right) + \\beta^2 Var (\\hat \\lambda) + 2 \\beta Cov \\left( (\\hat \\theta, \\hat \\lambda \\right)}^{\\frac{\\partial}{\\partial \\beta} Var\\left(\\hat \\theta + \\beta (\\hat \\lambda-\\lambda)\\right) \\,=\\, 0 \\; \\Rightarrow \\; 2\\beta Var(\\hat \\lambda ) +2 Cov(\\hat \\theta, \\hat \\lambda) \\,=\\,0} \n",
        "= \\; Var\\left(\\hat \\theta \\right) + \\left(-\\frac{Cov(\\hat \\theta, \\hat \\lambda)}{Var\\left(\\hat \\lambda \\right)}\\right)^2 Var\\left(\\hat \\lambda \\right) - 2\\frac{Cov(\\hat \\theta, \\hat \\lambda)}{Var\\left(\\hat \\lambda \\right)}Cov(\\hat \\theta, \\hat \\lambda) \\\\\n",
        "= {} & \\;  Var\\left(\\hat \\theta \\right) - \\frac{\\left(Cov(\\hat \\theta, \\hat \\lambda)\\right)^2}{Var\\left(\\hat \\lambda \\right)} < Var\\left(\\hat \\theta \\right)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where the above quantities can be emprically estimated from the samples in hand, and this method can be extended to include as many auxiliary estimators as are available with $\\hat \\theta + \\sum_{k=0}^K \\beta (\\hat \\lambda_k-\\lambda_k)$.\n",
        "\n",
        "> In importance sampling, it is known that $E[W_i^*] = 1$, \n",
        "> so a control variate estimator can be constructed as \n",
        "> $$\\overline{W^*Y} + \\beta (\\overline{W^*}-1)$$ \n",
        "> and while this will add bias of $\\mathcal{O}(\\frac{1}{n})$, the MSE of this estimator will eventually be better than that of $\\overline{W^*Y}$ for sufficiently large $n$."
      ],
      "metadata": {
        "id": "QGaOT73FKj1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's the above example using importance sampling; however\n",
        "# control variates are a general methodology \n",
        "# that can be used in contexts besides importance sampling\n",
        "\n",
        "# E[W*] = 1 is known; thus a control variate estimator is\n",
        "# \\bar{w*y} + \\beta (\\bar{w*} - 1)\n",
        "\n",
        "tilde_X = stats.uniform()\n",
        "X = stats.beta(2,2)\n",
        "n = 10000\n",
        "tilde_x = tilde_X.rvs(size=n) \n",
        "x = X.rvs(size=n)\n",
        "w = X.pdf(tilde_x)/tilde_X.pdf(tilde_x)\n",
        "\n",
        "def region(x, a=.95,b=.99):\n",
        "  return (x>a)&(x<b)\n",
        "\n",
        "print(\"True Value \", X.cdf(.99)-X.cdf(.95))\n",
        "print(\"Importance Sampling Estimate\")\n",
        "print(\"           \", (w*region(tilde_x)).mean())\n",
        "# Control Variates\n",
        "beta = -np.cov(w*region(tilde_x), w, ddof=1)[0,1]/np.var(w, ddof=1)\n",
        "# an optimal beta can be found as shown on top of the equation above\n",
        "print(\"Importance+Control Variate Sampling Estimate\")\n",
        "print(\"           \", (w*region(tilde_x)).mean() + (w.mean()-1)*beta)\n",
        "print(\"Control Variate and beta values: \", (w.mean()-1), beta)"
      ],
      "metadata": {
        "id": "1d_Gh5jTQMZD"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Importance Sampling Variance\")\n",
        "print(np.var(w*region(tilde_x)))\n",
        "print(\"Importance+Control Variate Sampling Variance\")\n",
        "# here's one way to estimate the variance of the Control Variate Sampling estimate\n",
        "tilde_cv_var = np.var(w*region(tilde_x), ddof=1) + \\\n",
        "               beta**2*np.var(1-w, ddof=1) + \\\n",
        "               2*beta*np.cov(w*region(tilde_x), w, ddof=1)[0,1]\n",
        "# or by the inequality derived above \n",
        "           # = np.var(w*region(tilde_x), ddof=1) - \\\n",
        "           #   np.cov(w*region(tilde_x), w, ddof=1)[0,1]**2/np.var(w, ddof=1)\n",
        "print(tilde_cv_var) # n factor to rescale to single sample"
      ],
      "metadata": {
        "id": "k2-IuoHOVsGm"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This example is inspired by Example 6.12 on page 190 in Chapter 6.4 **Variance Reduction Techniques** of the Givens and Hoeting **Computational Statistics** textbook*."
      ],
      "metadata": {
        "id": "D7hLluCzSGFK"
      }
    }
  ]
}